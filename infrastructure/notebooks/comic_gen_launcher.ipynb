{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI Comic Book Generator - Colab Launcher\n",
                "\n",
                "This notebook runs the AI Comic Generator **completely FREE** using local Ollama models.\n",
                "**No API keys needed!** Everything runs on Colab's free GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Setup Environment & Install Ollama\n",
                "# @markdown Enter your GitHub repo URL and branch.\n",
                "REPO_URL = \"https://github.com/yogesh-dixit-dev/comic-generator.git\" # @param {type:\"string\"}\n",
                "BRANCH = \"master\" # @param {type:\"string\"}\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Clone or update repo\n",
                "if not os.path.exists(\"comic-gen\"):\n",
                "    !git clone -b $BRANCH $REPO_URL comic-gen\n",
                "    %cd comic-gen\n",
                "else:\n",
                "    %cd /content/comic-gen\n",
                "    !git pull\n",
                "\n",
                "# Ensure requirements are installed\n",
                "!pip install -q -r requirements.txt\n",
                "\n",
                "# Install zstd (required by Ollama install script)\n",
                "print(\"üì¶ Installing dependencies (zstd)...\")\n",
                "!apt-get update -qq && apt-get install -y -qq zstd\n",
                "\n",
                "# Install Ollama\n",
                "print(\"üì¶ Installing Ollama...\")\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "\n",
                "# Start Ollama server in background\n",
                "print(\"üöÄ Starting Ollama server...\")\n",
                "!nohup ollama serve > ollama.log 2>&1 &\n",
                "\n",
                "# Pull the model\n",
                "import time\n",
                "time.sleep(5) # Wait for server\n",
                "print(\"‚¨áÔ∏è Downloading Llama 3.2 model (this may take a few minutes)...\")\n",
                "!ollama pull llama3.2\n",
                "\n",
                "print(\"\\n‚úÖ Environment Set Up! No API keys needed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Configuration (Optional - Hugging Face Storage)\n",
                "# @markdown Only needed if you want to save outputs to Hugging Face\n",
                "import os\n",
                "from google.colab import userdata\n",
                "\n",
                "# Try getting HF token from Colab Secrets\n",
                "try:\n",
                "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
                "    print(\"‚úÖ Loaded HF token from Colab Secrets\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è No HF_TOKEN in Colab Secrets. Output will stay local.\")\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Run Pipeline (100% Local - No API Calls!)\n",
                "# @markdown Upload your story file to the 'comic-gen' folder using the file browser on the left.\n",
                "INPUT_FILE = \"story.txt\" # @param {type:\"string\"}\n",
                "OUTPUT_DIR = \"output\" # @param {type:\"string\"}\n",
                "USE_HF_STORAGE = False # @param {type:\"boolean\"}\n",
                "HF_REPO_ID = \"your-username/comic-dataset\" # @param {type:\"string\"}\n",
                "\n",
                "# Make sure we're in the project directory\n",
                "import os\n",
                "if os.path.exists('/content/comic-gen'):\n",
                "    os.chdir('/content/comic-gen')\n",
                "    \n",
                "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
                "print(\"ü§ñ Using LOCAL Ollama (Llama 3.2) - No API calls!\\n\")\n",
                "\n",
                "# Build command\n",
                "storage_flag = f\"--storage hf --hf_repo '{HF_REPO_ID}'\" if USE_HF_STORAGE and os.environ.get('HF_TOKEN') else \"--storage local\"\n",
                "\n",
                "# Run the main script\n",
                "!python src/main.py --input \"$INPUT_FILE\" --output \"$OUTPUT_DIR\" --colab {storage_flag}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Download Output\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "if os.path.exists('output'):\n",
                "    !zip -r comic_output.zip output\n",
                "    files.download('comic_output.zip')\n",
                "    print(\"‚úÖ Download started!\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No output folder found. Did the pipeline run successfully?\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}